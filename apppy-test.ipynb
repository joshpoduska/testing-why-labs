{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fb991e-f9dd-434c-b52c-f48cba0a637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2203778-e569-437e-b94c-0fe6857ade6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "# from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3516048-3823-485e-bf86-4259704ddb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "862d5d1d-ec0b-46e9-8357-0045b8f0d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0,openai_api_key=OPENAI_API_KEY,model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df9dddb6-21fe-4629-890b-4b57034fb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_chat_llm(model,openai_api_key):\n",
    "    \n",
    "#     if model and openai_api_key:\n",
    "#         if model and model == \"GPT-3.5\":\n",
    "#             llm = ChatOpenAI(temperature=0,openai_api_key=OPENAI_API_KEY,model_name='gpt-3.5-turbo')\n",
    "#         elif model and model == \"GPT-4\":\n",
    "#             llm = llm = ChatOpenAI(temperature=0,openai_api_key=OPENAI_API_KEY,model_name='gpt-4')\n",
    "    \n",
    "#     return llm   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0a833b4-b342-4de4-bc2e-fcaefdd323f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_chat_llm(model, OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37d0a3d6-6a8b-47f5-927d-5e1d8f0af406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(vectorstore, model, openai_api_key):\n",
    "    \n",
    "    # qa_chain = None\n",
    "    # llm = None\n",
    "    \n",
    "    # if openai_api_key is None or not openai_api_key or openai_api_key == \"\":\n",
    "    #     openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    # if model and openai_api_key:\n",
    "    #     llm = get_chat_llm(model, openai_api_key)\n",
    " \n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    if llm:\n",
    "        qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory, qa_prompt=QA_PROMPT, condense_question_prompt=CONDENSE_QUESTION_PROMPT)\n",
    " \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00adea92-8b77-43ee-ae69-badcfbaabf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    " \n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a46cbfe6-0f97-4ecb-ad07-fafcb424511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14b9b29f-b168-445f-a1a7-414280675445",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI assistant for answering questions about information in Why Labs product documentation.\n",
    "You are given the following extracted parts of a long document and a question. Provide a conversational answer.\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "If the question is not about AI or ML or Observability or Monitoring or related to Why Labs, politely inform them that you \n",
    "are tuned to only answer questions about AI, ML, Observability, Monitoring and Why Labs.\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer in Markdown:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93e7abf9-c276-4229-acfe-36a56bfd958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e57d1fc6-23e8-4a3b-9ad8-1c797ce62e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b16dbc67-1ce1-4e9e-ac15-f200ab4a3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'store' not in locals() or store is None:\n",
    "    with open(\"WLDOCS.pkl\", \"rb\") as f:\n",
    "        store = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cdb529f-622c-40ae-a149-9ba7a6352d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ccfd776e-e262-41ea-ba14-47f116c453f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationalRetrievalChain\nqa_prompt\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mget_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPENAI_API_KEY\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 14\u001b[0m, in \u001b[0;36mget_chain\u001b[0;34m(vectorstore, model, openai_api_key)\u001b[0m\n\u001b[1;32m     12\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory(memory_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm:\n\u001b[0;32m---> 14\u001b[0m     qa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mConversationalRetrievalChain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQA_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondense_question_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONDENSE_QUESTION_PROMPT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qa_chain\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/chains/conversational_retrieval/base.py:356\u001b[0m, in \u001b[0;36mConversationalRetrievalChain.from_llm\u001b[0;34m(cls, llm, retriever, condense_question_prompt, chain_type, verbose, condense_question_llm, combine_docs_chain_kwargs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m _llm \u001b[38;5;241m=\u001b[39m condense_question_llm \u001b[38;5;129;01mor\u001b[39;00m llm\n\u001b[1;32m    350\u001b[0m condense_question_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m    351\u001b[0m     llm\u001b[38;5;241m=\u001b[39m_llm,\n\u001b[1;32m    352\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mcondense_question_prompt,\n\u001b[1;32m    353\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    354\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    355\u001b[0m )\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombine_docs_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondense_question_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain/load/serializable.py:75\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationalRetrievalChain\nqa_prompt\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "qa = get_chain(store, model_name, OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3b9fa93-0e45-4620-b7e1-527701b48327",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"what is why labs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e6de8df-1c98-4b94-816b-218c11db37c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_openai_callback() \u001b[38;5;28;01mas\u001b[39;00m cb:\n\u001b[0;32m----> 2\u001b[0m                 result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m                 answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "                result = qa({\"question\": user_input})\n",
    "                answer = result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06866b4-abea-4943-be12-7f80c90dbd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bdc7ee-f81c-46f0-a41f-e08d2af9552e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def25a3-b41b-490e-a1e8-e6947bdc7585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882df19f-80f0-4ca7-aea0-ba4cbb6afd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64a5eb-7c3d-40d3-9d14-6e016daa4d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11901a-cda8-4333-bcfb-afef5d400098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bbe520-3a55-4664-9118-722891f9f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import streamlit as st\n",
    " \n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from streamlit.web.server import websocket_headers\n",
    "from streamlit_chat import message\n",
    " \n",
    " \n",
    " \n",
    "def get_chat_llm(model,openai_api_key):\n",
    "    \n",
    "    if model and openai_api_key:\n",
    "        if model and model == \"GPT-3.5\":\n",
    "            llm = ChatOpenAI(temperature=0,openai_api_key=openai_api_key,model_name='gpt-3.5-turbo')\n",
    "        elif model and model == \"GPT-4\":\n",
    "            llm = llm = ChatOpenAI(temperature=0,openai_api_key=openai_api_key,model_name='gpt-4')\n",
    "    \n",
    "    return llm    \n",
    " \n",
    " \n",
    " \n",
    "def get_chain(vectorstore, model, openai_api_key):\n",
    "    \n",
    "    qa_chain = None\n",
    "    llm = None\n",
    "    \n",
    "    if openai_api_key is None or not openai_api_key or openai_api_key == \"\":\n",
    "        openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "    if model and openai_api_key:\n",
    "        llm = get_chat_llm(model, openai_api_key)\n",
    " \n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    if llm:\n",
    "        qa_chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), memory=memory, qa_prompt=QA_PROMPT, condense_question_prompt=CONDENSE_QUESTION_PROMPT)\n",
    " \n",
    "    return qa_chain\n",
    " \n",
    " \n",
    " \n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    " \n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    " \n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    " \n",
    "template = \"\"\"You are a friendly AI assistant for answering questions about information in Vanguards ETF documentation or Domino Data Labs product.\n",
    "You are given the following extracted parts of a long document and a question. Provide a conversational answer.\n",
    "If you don't know the answer, just say \"Hmm, I'm not sure.\" Don't try to make up an answer.\n",
    "If the question is not about investments, economics, finance, data science, AI or ML or MLOps or related to Vanguard or Domino Data Lab, politely inform them that you are tuned to only answer questions about a few areas.\n",
    "Question: {question}\n",
    "=========\n",
    "{context}\n",
    "=========\n",
    "Answer in Markdown:\"\"\"\n",
    " \n",
    "QA_PROMPT = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    " \n",
    "# Uncomment if you want to store and use the OpenAI key stored in an environment variable\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "# Initialise session state variables\n",
    "if 'generated' not in st.session_state:\n",
    "    st.session_state['generated'] = []\n",
    "if 'past' not in st.session_state:\n",
    "    st.session_state['past'] = []\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state['messages'] = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "if 'total_tokens' not in st.session_state:\n",
    "    st.session_state['total_tokens'] = []\n",
    " \n",
    "st.set_page_config(initial_sidebar_state='collapsed')\n",
    "openai_key = st.sidebar.text_input(\"Enter your OpenAI API key\", type=\"password\")\n",
    "model_name = st.sidebar.radio(\"Choose a model:\", (\"GPT-3.5\", \"GPT-4\"))\n",
    "docs_source = st.sidebar.radio(\"Choose a store:\", (\"DDL Docs\",\"ETF\"))\n",
    "clear_button = st.sidebar.button(\"Clear Conversation\", key=\"clear\")\n",
    " \n",
    " \n",
    "store = None \n",
    "if docs_source == \"ETF\":\n",
    "    # Load the embeddings from the pickle file; change the location if needed\n",
    "    with open(\"faiss_etf_doc_store.pkl\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "elif docs_source == \"DDL Docs\":\n",
    "    with open(\"faiss_ddl_doc_store.pkl\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    " \n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    " \n",
    "if clear_button:\n",
    "    st.session_state['generated'] = []\n",
    "    st.session_state['past'] = []\n",
    "    st.session_state['messages'] = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "    ]\n",
    "    memory.clear()\n",
    " \n",
    " \n",
    "if store:\n",
    "    qa = get_chain(store, model_name, openai_key)\n",
    " \n",
    " \n",
    "# container for chat history\n",
    "response_container = st.container()\n",
    "# container for text box\n",
    "container = st.container()\n",
    " \n",
    "with container:\n",
    "    with st.form(key='my_form', clear_on_submit=True):\n",
    "        user_input = st.text_area(\"You:\", key='input', height=100)\n",
    "        submit_button = st.form_submit_button(label='Send')\n",
    "    if submit_button and user_input and qa:\n",
    "        with st.spinner(\"Searching for the answer...\"):\n",
    "            with get_openai_callback() as cb:\n",
    "                result = qa({\"question\": user_input})\n",
    "                answer = result[\"answer\"]\n",
    " \n",
    "        st.session_state['total_tokens'].append(cb.total_tokens)\n",
    "        answer = result[\"answer\"]\n",
    "        st.session_state['past'].append(user_input)\n",
    "        st.session_state['generated'].append(answer)\n",
    "        \n",
    "    if st.session_state['generated']:\n",
    "        with response_container:\n",
    "            for i in range(len(st.session_state['generated'])):\n",
    "                message(st.session_state[\"past\"][i], is_user=True, logo='https://freesvg.org/img/1367934593.png', key=str(i) + '_user')\n",
    "                message(st.session_state[\"generated\"][i], logo='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQk6e8aarUy37BOHMTSk-TUcs4AyAy3pfAHL-F2K49KHNEbI0QUlqWJFEqXYQvlBdYMMJA&usqp=CAU', key=str(i))\n",
    "                if 'total_tokens' in st.session_state and len(st.session_state['total_tokens']) > 0:\n",
    "                    st.write(f\"Number of tokens: {st.session_state['total_tokens'][i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
