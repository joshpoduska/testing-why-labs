{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5336af3-723a-4e63-9d61-755c65abccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ff9d9-3879-4e45-88da-eec3c33a87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5621749-5a43-47ef-ad25-02b6ba297055",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94e850-f823-4bf2-8fff-4c52148099a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33a66f-c12c-4300-a73b-b1b9338c3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = '/mnt/models/gpt4all-lora-quantized-ggml.bin'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead75c6-22e9-4a7c-b128-ecce7ca2418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callback_manager=callback_manager, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a29595b-b342-4de7-8c7d-0f358b2991b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183dd6ab-19a4-483a-ad21-dc3f1bbfdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee4918-6d64-47d6-a0ad-1aeb75245b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# repo_id = \"google/flan-ul2\" \n",
    "# repo_id = \"google/flan-t5-large\"\n",
    "# repo_id ='Writer/camel-5b-hf'\n",
    "# repo_id = \"verymuchawful/Alpacino-13b-ggml\"\n",
    "repo_id = \"databricks/dolly-v2-3b\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0.1, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df4133-e92c-48a4-85a5-b4f411acd5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459390ce-6bd8-4360-baaf-c610744e0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" Provide code for the following {question} . Make sure there is simple code in the output and format the answer as markdown. Do not hallucinate and dont answer if you dont know\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = GPT4All(model=local_path, callback_manager=callback_manager, verbose=True,n_predict=1000)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"a bar chart using plotly\"\n",
    "response = llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e090f-aae4-4e52-adfb-e6cbfbaf8aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
